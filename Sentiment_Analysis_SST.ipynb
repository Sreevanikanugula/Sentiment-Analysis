{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmamxdSRjMZc"
      },
      "source": [
        "Mounting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X3Iv_4tUIXam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sOb33Vy6-eD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1247e7a5-290b-4f0a-d9aa-63cff6f247c9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e-JFLANjUDo"
      },
      "source": [
        "Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMrm7gPpoVCv"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "smx=SGDClassifier()\n",
        "lr=LogisticRegression(random_state = 0)\n",
        "knn = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "dt= DecisionTreeClassifier(criterion = \"gini\", random_state = 100,\n",
        "                               max_depth=2, min_samples_leaf=5)\n",
        "km=KMeans(n_clusters=2)\n",
        "rf = RandomForestClassifier(n_estimators=100, criterion = 'entropy', random_state = 0)\n",
        "sv=SVC(kernel='poly')\n",
        "xgb=XGBClassifier()\n",
        "gb=GradientBoostingClassifier()\n",
        "gnb=GaussianNB()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yudzAd-japQ"
      },
      "source": [
        "Py-Torch,sentence embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXHuDBSGj8Au",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ee34225-22cb-48e6-ee34-5b318e1d1230"
      },
      "source": [
        "!pip install pytorch-pretrained-bert\n",
        "import torch\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
        "\n",
        "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
        "import logging\n",
        "#logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "text = \"Here is the sentence I want embeddings for.\"\n",
        "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "\n",
        "# Tokenize our sentence with the BERT tokenizer.\n",
        "tokenized_text = tokenizer.tokenize(marked_text)\n",
        "\n",
        "# Print out the tokens.\n",
        "print (tokenized_text)\n",
        "# Define a new example sentence with multiple meanings of the word \"bank\"\n",
        "text = \"After stealing money from the bank vault, the bank robber was seen \" \\\n",
        "       \"fishing on the Mississippi river bank.\"\n",
        "\n",
        "# Add the special tokens.\n",
        "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "\n",
        "# Split the sentence into tokens.\n",
        "tokenized_text = tokenizer.tokenize(marked_text)\n",
        "\n",
        "# Map the token strings to their vocabulary indeces.\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "# Display the words with their indeces.\n",
        "for tup in zip(tokenized_text, indexed_tokens):\n",
        "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))\n",
        "    # Mark each of the 22 tokens as belonging to sentence \"1\".\n",
        "segments_ids = [1] * len(tokenized_text)\n",
        "\n",
        "print (segments_ids)\n",
        "# Convert inputs to PyTorch tensors\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "# Load pre-trained model (weights)\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    encoded_layers, _ = model(tokens_tensor, segments_tensors)\n",
        "print (\"Number of layers:\", len(encoded_layers))\n",
        "layer_i = 0\n",
        "\n",
        "print (\"Number of batches:\", len(encoded_layers[layer_i]))\n",
        "batch_i = 0\n",
        "\n",
        "print (\"Number of tokens:\", len(encoded_layers[layer_i][batch_i]))\n",
        "token_i = 0\n",
        "\n",
        "print (\"Number of hidden units:\", len(encoded_layers[layer_i][batch_i][token_i]))\n",
        "\n",
        "# Concatenate the tensors for all layers. We use `stack` here to\n",
        "# create a new dimension in the tensor.\n",
        "token_embeddings = torch.stack(encoded_layers, dim=0)\n",
        "\n",
        "token_embeddings.size()\n",
        "# Remove dimension 1, the \"batches\".\n",
        "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "\n",
        "token_embeddings.size()\n",
        "# Swap dimensions 0 and 1.\n",
        "token_embeddings = token_embeddings.permute(1,0,2)\n",
        "\n",
        "token_embeddings.size()\n",
        "# Stores the token vectors, with shape [22 x 3,072]\n",
        "token_vecs_cat = []\n",
        "\n",
        "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
        "\n",
        "# For each token in the sentence...\n",
        "for token in token_embeddings:\n",
        "    \n",
        "    # `token` is a [12 x 768] tensor\n",
        "\n",
        "    # Concatenate the vectors (that is, append them together) from the last \n",
        "    # four layers.\n",
        "    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
        "    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
        "    \n",
        "    # Use `cat_vec` to represent `token`.\n",
        "    token_vecs_cat.append(cat_vec)\n",
        "\n",
        "print ('Shape is: %d x %d' % (len(token_vecs_cat), len(token_vecs_cat[0])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.7/dist-packages (0.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (4.41.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.17.101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.9.0+cu102)\n",
            "Requirement already satisfied: botocore<1.21.0,>=1.20.101 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (1.20.101)\n",
            "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (0.4.2)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (0.10.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.7.4.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.101->boto3->pytorch-pretrained-bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.101->boto3->pytorch-pretrained-bert) (1.15.0)\n",
            "['[CLS]', 'here', 'is', 'the', 'sentence', 'i', 'want', 'em', '##bed', '##ding', '##s', 'for', '.', '[SEP]']\n",
            "[CLS]           101\n",
            "after         2,044\n",
            "stealing     11,065\n",
            "money         2,769\n",
            "from          2,013\n",
            "the           1,996\n",
            "bank          2,924\n",
            "vault        11,632\n",
            ",             1,010\n",
            "the           1,996\n",
            "bank          2,924\n",
            "robber       27,307\n",
            "was           2,001\n",
            "seen          2,464\n",
            "fishing       5,645\n",
            "on            2,006\n",
            "the           1,996\n",
            "mississippi   5,900\n",
            "river         2,314\n",
            "bank          2,924\n",
            ".             1,012\n",
            "[SEP]           102\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Number of layers: 12\n",
            "Number of batches: 1\n",
            "Number of tokens: 22\n",
            "Number of hidden units: 768\n",
            "Shape is: 22 x 3072\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DRNI1YcJU6F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34cd6be0-f81e-481b-8c65-ce0f8efb9431"
      },
      "source": [
        "import torch\n",
        "torch.Size([12, 1, 22, 768])\n",
        "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "\n",
        "token_embeddings.size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([22, 12, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mo6Icaivjnbs"
      },
      "source": [
        "Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbqjlM1ix9M5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44bbe127-1e8b-4c6d-acc7-dc5bb5b43a22"
      },
      "source": [
        "!pip install pytreebank\n",
        "\n",
        "\n",
        "import pytreebank\n",
        "sst = pytreebank.load_sst()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytreebank in /usr/local/lib/python3.7/dist-packages (0.2.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lj_ynly1VakE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "outputId": "28e3f39a-efd8-4f0d-e96d-cbaa01cf5824"
      },
      "source": [
        "import pandas as pd\n",
        "data_train = [\n",
        "                (\n",
        "                    tree.to_lines()[0],\n",
        "                    tree.label\n",
        "                )\n",
        "                for tree in sst['train']\n",
        "            ]\n",
        "data_test = [(\n",
        "  tree.to_lines()[0],\n",
        "  tree.label\n",
        ")\n",
        "for tree in sst['test']\n",
        "]\n",
        "x_train=pd.DataFrame(data_train,columns=['sentence','label'])\n",
        "x_test=pd.DataFrame(data_test,columns=['sentence','label'])\n",
        "x_train_sent=x_train[['sentence']]\n",
        "print(x_train_sent)\n",
        "y_train_ind=x_train['label']\n",
        "y_train_ind\n",
        "x_test_sent=x_test[['sentence']]\n",
        "y_test_ind=x_test['label']\n",
        "x_test_sent\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                               sentence\n",
            "0     The Rock is destined to be the 21st Century 's...\n",
            "1     The gorgeously elaborate continuation of `` Th...\n",
            "2     Singer/composer Bryan Adams contributes a slew...\n",
            "3     You 'd think by now America would have had eno...\n",
            "4                  Yet the act is still charming here .\n",
            "...                                                 ...\n",
            "8539                                    A real snooze .\n",
            "8540                                     No surprises .\n",
            "8541  We 've seen the hippie-turned-yuppie plot befo...\n",
            "8542  Her fans walked out muttering words like `` ho...\n",
            "8543                                In this case zero .\n",
            "\n",
            "[8544 rows x 1 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Effective but too-tepid biopic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>If you sometimes like to go to the movies to h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Emerges as something rare , an issue movie tha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The film provides some great insight into the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Offers that rare combination of entertainment ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2205</th>\n",
              "      <td>An imaginative comedy/thriller .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2206</th>\n",
              "      <td>( A ) rare , beautiful film .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2207</th>\n",
              "      <td>( An ) hilarious romantic comedy .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2208</th>\n",
              "      <td>Never ( sinks ) into exploitation .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2209</th>\n",
              "      <td>( U ) nrelentingly stupid .</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2210 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence\n",
              "0                        Effective but too-tepid biopic\n",
              "1     If you sometimes like to go to the movies to h...\n",
              "2     Emerges as something rare , an issue movie tha...\n",
              "3     The film provides some great insight into the ...\n",
              "4     Offers that rare combination of entertainment ...\n",
              "...                                                 ...\n",
              "2205                   An imaginative comedy/thriller .\n",
              "2206                      ( A ) rare , beautiful film .\n",
              "2207                 ( An ) hilarious romantic comedy .\n",
              "2208                Never ( sinks ) into exploitation .\n",
              "2209                        ( U ) nrelentingly stupid .\n",
              "\n",
              "[2210 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVJUW_aij3vn"
      },
      "source": [
        "Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGrATjN7kr7R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "431c6696-cec1-4054-f2c1-103290123029"
      },
      "source": [
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "tokenizer = Tokenizer(num_words=2500, lower=True,split=' ')\n",
        "tokenizer.fit_on_texts(x_train_sent['sentence'])\n",
        "tokenizer.fit_on_texts(x_test_sent['sentence'])\n",
        "# print(len(tokenizer.word_index))\n",
        "X = tokenizer.texts_to_sequences(list(x_train_sent['sentence']))\n",
        "x=tokenizer.texts_to_sequences(list(x_test_sent['sentence']))\n",
        "x=pad_sequences(x)\n",
        "X = pad_sequences(X)\n",
        "print(X.size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "393024\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roAb8ab6j8Xr"
      },
      "source": [
        "Vocabulary "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3TMQx7-d1VO"
      },
      "source": [
        "import pickle\n",
        "# embeding_matrix=np.zeros((len(),300))\n",
        "vocabulary=[]\n",
        "for token,index in tokenizer.word_index.items():\n",
        "  vocabulary.insert(index,token) \n",
        "#pickle.dump(vocabulary,open('/content/drive/MyDrive/vocabulary.pickle', 'wb'))\n",
        "# for wor in vocabulary:\n",
        "# len(vocabulary)\n",
        "# len(tokenizer.word_index.keys())  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VANr_V5M7acB"
      },
      "source": [
        "import pickle\n",
        "vocabulary=pickle.load(open('/content/drive/MyDrive/vocabulary.pickle', 'rb'))\n",
        "glove_embed_sst=pickle.load(open('/content/drive/MyDrive/Glove_embed_sst.pkl', 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iC959u4kDaY"
      },
      "source": [
        "Embedding matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFbQFBnzNmH7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dd395e9-234c-41dd-99bc-8fe6e4f2c189"
      },
      "source": [
        "embedding_matrix=np.zeros((len(vocabulary),300))\n",
        "for word,vec in glove_embed_sst.items():\n",
        "  ind=vocabulary.index(word)\n",
        "  embedding_matrix[ind]=vec\n",
        "embedding_matrix.shape\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17087, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPNKy9nckHdb"
      },
      "source": [
        "LSTM and Bi-LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOTABNes0WYk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b9bcdd6-1a2c-4613-98d0-55bbf0266b8b"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout, Masking, Embedding,Bidirectional\n",
        "from keras.layers import Dense\n",
        "lstm_out = 300\n",
        "batch_size = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(vocabulary), 300, trainable = False, weights=[embedding_matrix]))\n",
        "#model.add(Embedding(len(vocabulary), 300,input_length = X.shape[1]))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Masking(mask_value=0))\n",
        "#model.add(Bidirectional(LSTM(lstm_out, recurrent_dropout = 0.2, dropout = 0.2)))\n",
        "model.add(LSTM(lstm_out, recurrent_dropout = 0.2, dropout = 0.2))\n",
        "# model.add(Dropout(0.2))\n",
        "model.add(Dense(5,activation='softmax'))\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 300)         5126100   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, None, 300)         0         \n",
            "_________________________________________________________________\n",
            "masking (Masking)            (None, None, 300)         0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 300)               721200    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 5)                 1505      \n",
            "=================================================================\n",
            "Total params: 5,848,805\n",
            "Trainable params: 722,705\n",
            "Non-trainable params: 5,126,100\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGl0L31lbpDD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0616948d-8767-4b12-c57f-b2b93532f4b2"
      },
      "source": [
        "ohe = OneHotEncoder()\n",
        "Y = ohe.fit_transform(np.array(y_train_ind).reshape(-1,1)).toarray()\n",
        "y = ohe.fit_transform(np.array(y_test_ind).reshape(-1,1)).toarray()\n",
        "model.fit(X,Y, batch_size =batch_size, epochs = 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "267/267 [==============================] - 162s 507ms/step - loss: 1.5874 - accuracy: 0.2604\n",
            "Epoch 2/5\n",
            "267/267 [==============================] - 136s 509ms/step - loss: 1.5175 - accuracy: 0.3118\n",
            "Epoch 3/5\n",
            "267/267 [==============================] - 133s 497ms/step - loss: 1.4758 - accuracy: 0.3495\n",
            "Epoch 4/5\n",
            "267/267 [==============================] - 135s 507ms/step - loss: 1.4307 - accuracy: 0.3838\n",
            "Epoch 5/5\n",
            "267/267 [==============================] - 135s 506ms/step - loss: 1.3747 - accuracy: 0.4125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9631460bd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTUaHlFNTVcQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a385010-ac18-4016-b9d1-9f239997807b"
      },
      "source": [
        " score,evalu=model.evaluate(x,y,verbose=2,batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "70/70 - 7s - loss: 1.4954 - accuracy: 0.3430\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6akF0gqWXb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1339cb11-3329-45f7-ed66-84afbf2c1fe1"
      },
      "source": [
        " print(evalu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.34298643469810486\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MT6lugWkUmx"
      },
      "source": [
        "BERT embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dz8htZUtYAnK",
        "outputId": "638666c0-afb9-4197-a4b6-66cbb522daab"
      },
      "source": [
        "!pip install sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/fd/8a81047bbd9fa134a3f27e12937d2a487bd49d353a038916a5d7ed4e5543/sentence-transformers-2.0.0.tar.gz (85kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 4.2MB/s \n",
            "\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/1a/41c644c963249fd7f3836d926afa1e3f1cc234a1c40d80c5f03ad8f6f1b2/transformers-4.8.2-py3-none-any.whl (2.5MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5MB 8.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.9.0+cu102)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.10.0+cu102)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/aa/1437691b0c7c83086ebb79ce2da16e00bef024f24fec2a5161c35476f499/sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 26.9MB/s \n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "  Downloading https://files.pythonhosted.org/packages/35/03/071adc023c0a7e540cf4652fa9cad13ab32e6ae469bf0cc0262045244812/huggingface_hub-0.0.13-py3-none-any.whl\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 45.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.5.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 37.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.13)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (20.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.5.30)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.4.7)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.0.0-cp37-none-any.whl size=126711 sha256=531e87b845421084ef19761905f75d8348e99140d2c10a7dde9c8698ed92dbf6\n",
            "  Stored in directory: /root/.cache/pip/wheels/38/d2/98/d191289a877a34c68aa67e05179521e060f96394a3e9336be6\n",
            "Successfully built sentence-transformers\n",
            "\u001b[31mERROR: transformers 4.8.2 has requirement huggingface-hub==0.0.12, but you'll have huggingface-hub 0.0.13 which is incompatible.\u001b[0m\n",
            "Installing collected packages: huggingface-hub, tokenizers, sacremoses, transformers, sentencepiece, sentence-transformers\n",
            "Successfully installed huggingface-hub-0.0.13 sacremoses-0.0.45 sentence-transformers-2.0.0 sentencepiece-0.1.96 tokenizers-0.10.3 transformers-4.8.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-2J3XoEWF92"
      },
      "source": [
        "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "#sentences = ['This framework generates embeddings for each input sentence',  'Sentences are passed as a list of string.', 'The quick brown fox jumps over the lazy dog.']\n",
        "# sentence_embeddings = model.encode(x_train_sent['sentence'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cX9ApF1A26OK"
      },
      "source": [
        "sentence_embeddings_test=model.encode(x_test_sent['sentence'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYi8L4cFkZhE"
      },
      "source": [
        "Sentence Embeddings of train and test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Y_nUeTdIXA1"
      },
      "source": [
        "# pickle.dump(sentence_embeddings_test,open('/content/drive/My Drive/embeding_test.pickle', 'wb'))\n",
        "# pickle.dump(sentence_embeddings,open('/content/drive/My Drive/embeding.pickle', 'wb'))\n",
        "# !cat /content/drive/My\\ Drive/f.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-R-_41o6rPY0"
      },
      "source": [
        "import pickle\n",
        "sentence_embeddings=pickle.load(open('/content/drive/MyDrive/embeding.pickle', 'rb'))\n",
        "len(sentence_embeddings)\n",
        "sentence_embeddings_test=pickle.load(open('/content/drive/MyDrive/embeding_test.pickle','rb'))\n",
        "#print(sentence_embeddings_test[0].size)\n",
        "#print(x_train_sent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyLxQKnWkiaq"
      },
      "source": [
        "Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhM1oF2ppvrL"
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "# sc = StandardScaler()\n",
        "# X = sc.fit_transform(X)\n",
        "ohe = OneHotEncoder()\n",
        "y_test_ind = ohe.fit_transform(y_sst2_tes.reshape(-1,1)).toarray()\n",
        "y_train_ind = ohe.fit_transform(y_sst2_t.reshape(-1,1)).toarray()\n",
        "# X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.1)\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=(768), activation='relu'))\n",
        "# model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.0001), metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4Ar9xg12BER",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbfd5e2b-d406-4372-c0aa-e7613e6288fe"
      },
      "source": [
        "np.array(sentence_embeddings_test).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2210, 768)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0OL4MM7vAn1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70b26214-3dfb-4027-b010-454b07c0a403"
      },
      "source": [
        "history = model.fit(np.array(sentence_embeddings), y_train_ind, epochs=10, batch_size=32)\n",
        "y_pred = model.predict(np.array(sentence_embeddings_test))\n",
        "y_pred=np.array(y_pred)\n",
        "pred = list()\n",
        "for i in range(len(y_pred)):\n",
        "    pred.append(np.argmax(y_pred[i]))\n",
        "test = list()\n",
        "for i in range(len(y_test_ind)):\n",
        "    test.append(np.argmax(y_test_ind[i]))\n",
        "a = accuracy_score(pred,test)\n",
        "print('Accuracy is:', a*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "267/267 [==============================] - 5s 2ms/step - loss: 0.5184 - accuracy: 0.7439\n",
            "Epoch 2/10\n",
            "267/267 [==============================] - 1s 2ms/step - loss: 0.3879 - accuracy: 0.8262\n",
            "Epoch 3/10\n",
            "267/267 [==============================] - 1s 2ms/step - loss: 0.3639 - accuracy: 0.8412\n",
            "Epoch 4/10\n",
            "267/267 [==============================] - 1s 2ms/step - loss: 0.3556 - accuracy: 0.8403\n",
            "Epoch 5/10\n",
            "267/267 [==============================] - 1s 2ms/step - loss: 0.3405 - accuracy: 0.8509\n",
            "Epoch 6/10\n",
            "267/267 [==============================] - 1s 2ms/step - loss: 0.3336 - accuracy: 0.8549\n",
            "Epoch 7/10\n",
            "267/267 [==============================] - 1s 2ms/step - loss: 0.3289 - accuracy: 0.8609\n",
            "Epoch 8/10\n",
            "267/267 [==============================] - 1s 2ms/step - loss: 0.3210 - accuracy: 0.8607\n",
            "Epoch 9/10\n",
            "267/267 [==============================] - 1s 2ms/step - loss: 0.3215 - accuracy: 0.8573\n",
            "Epoch 10/10\n",
            "267/267 [==============================] - 1s 2ms/step - loss: 0.3113 - accuracy: 0.8673\n",
            "Accuracy is: 84.16289592760181\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fifxeIT3kmfw"
      },
      "source": [
        "Text-Blob"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZE_QXbNQ4YhJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "236e2340-f307-4dbb-c892-a19518fe583e"
      },
      "source": [
        "from textblob import TextBlob\n",
        "y_pr=[]\n",
        "for sentence in list(x_test_sent['sentence']):\n",
        "  y_pr.append(TextBlob(sentence).sentiment.polarity)\n",
        "y_pr\n",
        "df=pd.DataFrame(y_pr,columns=['sent'])\n",
        "df['sent']=pd.cut(df['sent'],bins=5,labels=[0,1,2,3,4])\n",
        "accuracy_score(x_test['label'],df['sent'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.283710407239819"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ue6ajp6ukqWE"
      },
      "source": [
        "Vader Lexicon"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ0mF7kGyY6j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7230639-3da9-4c88-e3fe-b603113b2a59"
      },
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "vader=SentimentIntensityAnalyzer()\n",
        "y_pr=[]\n",
        "for sentence in list(x_test_sent['sentence']):\n",
        "  y_pr.append(vader.polarity_scores(sentence)['compound'])\n",
        "y_pr\n",
        "df=pd.DataFrame(y_pr,columns=['sent'])\n",
        "df['sent']=pd.cut(df['sent'],bins=2,labels=[0, 1])\n",
        "accuracy_score(x_test['label'],df['sent'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.20950226244343892"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axOxu9wDYfSH"
      },
      "source": [
        "ML CLASSIFIERS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdVrJUZ_YeXB",
        "outputId": "3527d3a9-3e98-4c9c-f816-fb688ce69f9f"
      },
      "source": [
        "#lr.fit(np.array(sentence_embeddings),np.array(y_train_ind))\n",
        "#lr.fit(np.array(sentence_embeddings),y_sst2_t)\n",
        "#y_pred=lr.predict(np.array(sentence_embeddings_test))\n",
        "#y_pred=lr.predict(np.array(sentence_embeddings_test))\n",
        "#accuracy_score(np.array(y_test_ind),np.array(y_pred))\n",
        "#accuracy_score(y_sst2_tes,np.array(y_pred))\n",
        "\n",
        "#xgb.fit(np.array(sentence_embeddings),y_sst2_t)\n",
        "#y_pred=xgb.predict(np.array(sentence_embeddings_test))\n",
        "#accuracy_score(y_sst2_tes,np.array(y_pred))\n",
        "\n",
        "#gb.fit(np.array(sentence_embeddings),y_sst2_t)\n",
        "#y_pred=gb.predict(np.array(sentence_embeddings_test))\n",
        "#accuracy_score(y_sst2_tes,np.array(y_pred))\n",
        "\n",
        "#knn.fit(np.array(sentence_embeddings),np.array(y_train_ind))\n",
        "\n",
        "rf.fit(np.array(sentence_embeddings),np.array(y_train_ind))\n",
        "y_pred=rf.predict(np.array(sentence_embeddings_test))\n",
        "accuracy_score(y_test_ind,y_pred)\n",
        "\n",
        "#y_pred=rf.predict(np.array(sentence_embeddings_test))\n",
        "#smx.fit(np.array(sentence_embeddings),np.array(y_train_ind))\n",
        "#gnb.fit(np.array(sentence_embeddings),np.array(y_train_ind))\n",
        "\n",
        "# l=[(r,z) for r,z in zip(y_test_ind,y_pred)]\n",
        "# print(tuple(x)).[0:10]\n",
        "# print(l)\n",
        "# print(y_train_ind)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.46289592760181"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oizdy1TktVg"
      },
      "source": [
        "Converting SST-5 to SST-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-t7Aat3sPiWG"
      },
      "source": [
        "def convert(n):\n",
        "  if(n>2):\n",
        "    n=1\n",
        "  else:\n",
        "    n=0\n",
        "  return n "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZM7VGEhx4zJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6b816f6-ca56-4b20-99bd-cb94c20e5f17"
      },
      "source": [
        "y_train_ind=np.array(y_train_ind)\n",
        "y_test_ind=np.array(y_test_ind)\n",
        "y_sst2_t=[]\n",
        "y_sst2_tes=[]\n",
        "for i in y_test_ind:\n",
        "  y_sst2_tes.append(convert(i))\n",
        "for i in y_train_ind:\n",
        "  y_sst2_t.append(convert(i))\n",
        "y_sst2_t=np.array(y_sst2_t)\n",
        "y_sst2_tes=np.array(y_sst2_tes)\n",
        "y_sst2_t\n",
        "y_sst2_t.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8544,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    }
  ]
}